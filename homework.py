# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OTB5DHBxUrTKeGrO-pgQ9z0Kda3h-eMy
"""

pip install jupyter

pip install requests

pip install beautifulsoup4

pip install scrapy

from urllib.request import urlopen
html = urlopen('https://en.wikipedia.org/wiki/Python')
print(html.read())

import requests
data = requests.get('https://www.herzen.spb.ru/main/1')
print(data.status_code)
if(data.status_code !=200):
  text =(data.content)
  print(text)

from urllib.request import urlopen
from bs4 import BeautifulSoup
html = urlopen('https://en.wikipedia.org/wiki/Python')
bs = BeautifulSoup(html.read(),'html.parser')# без .read(), если работаем с файлом
print(bs.h1)

pip install lxml

from urllib.request import urlopen
from urllib.error import HTTPError
from urllib.error import URLError
# аналогично обрабатываются ошибки при использовании библиотеки requests,
# см. документацию тут: https://docs.python-requests.org/en/latest/
try:
	html = urlopen("https://www.google.com")
except HTTPError as e:
	print("The server returned an HTTP error")
except URLError as e:
	print("The server could not be found!")
else :
	print(html.read())

from urllib.request import urlopen
from urllib.error import HTTPError
from bs4 import BeautifulSoup
def getTitle(url):
	try:
		html = urlopen(url)
	except HTTPError as e:
		return None
	try:
		bsObj = BeautifulSoup(html.read(),'html.parser')
		title = bsObj.body.h1
	except AttributeError as e:
		return None
	return title
title = getTitle("https://en.wikipedia.org/wiki/Python")
if title ==None:
	print("Title could not be found")
else:
	print(title)

"""Here My Spider Code:
I declare class to save the data which I read on it
"""

from urllib.request import urlopen
from bs4 import BeautifulSoup
class ParagraphSpider:
    def __init__(self, title, content):
        self.title = title
        self.content = content
    def to_dict(self):
        return {
            'title': self.title,
            'content': self.content,
        }
article=[] #article titles

html = urlopen('https://habr.com/ru/articles/767750/')
bs = BeautifulSoup(html,"html.parser")
nameList = bs.findAll('h2')
# for i in range(10) # for (let i=0; i < 10; i++){}
for name in nameList:
  #print(name.get_text())
  content=''
  for s in name.find_next_siblings():
        if s.name == 'p':
            #print(s.get_text(strip=True))
            content +=s.get_text(strip=True)

        else:
            #print('-----------------')
            article.append(ParagraphSpider(name.get_text(),content))
            break
  #title.append(name.get_text())

"""then show the array"""

for s in article:
  print('-->',s.title)
  print(s.content)

article.clear()

"""function to save the areray on csv file"""

import pandas as pd
def output(csv_name,article):
  df = pd.DataFrame.from_records([s.to_dict() for s in article])
#  for s in article:
#      df['title'] = s.title
#      df['content'] = s.content
#      print('-->',s.title)
#      print(s.content)
  df.to_csv(csv_name, encoding='utf_8_sig')  # save to csv

"""call the function"""

output(csv_name="Result.csv",article=article)

from urllib.request import urlopen
from bs4 import BeautifulSoup
html = urlopen('https://habr.com/ru/companies/globalsign/articles/425171/')
bs = BeautifulSoup(html,'html.parser')
#print(bs)
# Сравнить:
for desc in bs.find('h2').descendants:
  print('->', desc)
# с этим:
# for child in bs.find('table',{'id':'giftList'}).children:
# print(child)

from urllib.request import urlopen
from bs4 import BeautifulSoup
import re
html = urlopen('http://www.pythonscraping.com/pages/page3.html')
bs = BeautifulSoup(html,'html.parser')
images = bs.find_all('img', {'src':re.compile('\.\.\/img\/gifts/img.*\.jpg')})
for image in images:
  print (image['src'])

import csv
def csv_writer(data,path):
  """
  Write data to a CSV file path
  """
  with open(path,"w", newline='') as csv_file:
    writer = csv.writer(csv_file, delimiter=',')
    for line in data:
      writer.writerow(line)
data = ["url,author,article_title".split(","),
            "Tyrese,Hirthe,Strackeport".split(","),
            "Jules,Dicki,Lake Nickolasville".split(","),
            "Dedric,Medhurst,Stiedemannberg".split(",")
            ]
path ="output.csv"
csv_writer(data, path)

pip install Scrapy

import scrapy
class ArticleSpider(scrapy.Spider):
  name='article'
  def start_requests(self):
    urls = ["http://en.wikipedia.org/wiki/Python_%28programming_language%29","https://en.wikipedia.org/wiki/Functional_programming","https://en.wikipedia.org/wiki/Monty_Python"]
    return [scrapy.Request(url=url, callback=self.parse) for url in urls]
  def parse(self, response):
    url = response.url
    title = response.css('h1::text').extract_first()
    print('URL is: {}'.format(url))
    print('Title is: {}'.format(title))

article =ArticleSpider()

#response.url="https://habr.com/ru/companies/globalsign/articles/425171/"
for response in article.start_requests():
  article.parse(response)